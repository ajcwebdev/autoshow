// src/llms/together.ts

import { err } from '../utils/logging.ts'
import { env } from '../utils/node-utils.ts'
import { LLM_SERVICES_CONFIG } from '../../shared/constants.ts'
import { checkLLMApiKey, buildCombinedPrompt } from '../utils/llm-service-utils.ts'

export type TogetherModelValue = (typeof LLM_SERVICES_CONFIG.together.models)[number]['modelId']

export async function callTogether(
  prompt: string,
  transcript: string,
  modelValue: TogetherModelValue
) {
  // Previously: if (!env['TOGETHER_API_KEY']) { ... } throw ...
  checkLLMApiKey('together')

  const combinedPrompt = buildCombinedPrompt(prompt, transcript)

  const requestBody = {
    model: modelValue,
    max_tokens: 4000,
    messages: [{ role: 'user', content: combinedPrompt }]
  }
  try {
    const response = await fetch('https://api.together.xyz/v1/chat/completions', {
      method: 'POST',
      headers: {
        Accept: 'application/json',
        'Content-Type': 'application/json',
        Authorization: `Bearer ${env['TOGETHER_API_KEY']}`
      },
      body: JSON.stringify(requestBody)
    })
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`Together AI API error: ${response.status} ${response.statusText} - ${errorText}`)
    }
    const data = await response.json()
    const content = data.choices?.[0]?.message?.content
    if (!content) {
      throw new Error('No content generated by the Together AI API.')
    }
    return {
      content,
      usage: {
        stopReason: data.choices?.[0]?.finish_reason ?? 'unknown',
        input: data.usage?.prompt_tokens,
        output: data.usage?.completion_tokens,
        total: data.usage?.total_tokens
      }
    }
  } catch (error) {
    err(`Error in callTogether: ${(error as Error).message}`)
    throw error
  }
}
