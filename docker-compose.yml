# docker-compose.yml

services:
  # Main application service
  autoshow:
    # Build configuration for this service
    build:
      # Use the current directory as build context
      context: .
      # Use the Dockerfile in the root directory
      dockerfile: Dockerfile
    # Mount these directories/files from host to container
    volumes:
      # Share content directory between host and container
      - ./content:/usr/src/app/content
      # Mount environment variables file
      - ./.env:/usr/src/app/.env
      # Allow container to interact with Docker daemon
      - /var/run/docker.sock:/var/run/docker.sock
    # Specify services that must start before this one
    depends_on:
      # Depends on the whisper service
      - whisper
      # Depends on the ollama service
      - ollama
    # Set environment variables for this container
    environment:
      # Configure Ollama host to point to the ollama service
      - OLLAMA_HOST=ollama
    # Connect to the autoshownet network
    networks:
      - autoshownet

  # Speech-to-text service using whisper.cpp
  whisper:
    # Build configuration for whisper service
    build:
      # Use whisper.cpp directory as build context
      context: ./whisper.cpp
      # Use the Dockerfile in the whisper.cpp directory
      dockerfile: Dockerfile
    # Mount these directories between host and container
    volumes:
      # Share content directory
      - ./content:/app/content
      # Use a named volume for models
      - whisper-models:/app/models
    # Keep container running (placeholder command)
    command: tail -f /dev/null
    # Allocate a pseudo-TTY
    tty: true
    # Keep STDIN open
    stdin_open: true
    # Connect to the autoshownet network
    networks:
      - autoshownet

  # Large language model service
  ollama:
    # Use the official Ollama image
    image: ollama/ollama
    # Mount these volumes
    volumes:
      # Use a named volume for models
      - ollama-models:/root/.ollama
    # Set environment variables
    environment:
      # Make Ollama accessible on all network interfaces
      - OLLAMA_HOST=0.0.0.0
    # Connect to the autoshownet network
    networks:
      - autoshownet

# Define networks used by the services
networks:
  # Custom network for internal communication
  autoshownet:
    # Use bridge network driver (standard Docker network type)
    driver: bridge

# Define named volumes used by the services
volumes:
  ollama-models:
  whisper-models: